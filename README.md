# Click-Through-Rate-Prediction
First preprocessing messy data from Kaggle, then use auto-modeling statistical methods to predict the click-through rate 
# Summary Online advertisements have a significant influence on the success of a business. Effective advertisements can help businesses establish long-term relationships with customers and target the right potential customers, resulting in repeat sales and a high conversion rate. Therefore, our project is aimed to forecast the click-through rate (CTR) for evaluating ad performance and identifying potential users. The process comprises four stages: gathering data, pre-processing, training and testing the model, and evaluation. We use the data present on Kaggle and then convert the raw data by partitioning, recategorizing, etc. After that, we use the cleaned data to train predictive models. Finally, experimental results reveal that our best performance model produces an accuracy of 65.2% and is good at identifying both clicked and non-clicked ads. Time is the most influential factor, followed by the types of device model, website and app in predicting click-through rate.
# Methodology Make possible of our ensemble approach:
1.	Test different algorithms on a small subset of data to decide which ones to use for our ensemble model. Tested algorithms include logistic regression, naive bayes, KNN, classification tree, bagging, random forest, gradient boosting, XGB. Decide to use logistic regression, naive bayes and classification tree as our base models and build both homogeneous & heterogeneous ensemble algorithms as our final models for later testing. KNN is not chosen because the computational requirement at scale is too large. Random forest is preferred for our developed ensemble model because of the relatively good performance on unbalanced test data. 
2.	Develop our own programs to automate the preprocessing, sampling and modeling procedures, which require extremely expensive human power and are not realistic to finish without automated machines. 
3.	Develop our own ensemble algorithm that randomly chooses 2 to 6 features as predictors on base models to de-correlate the models, detect the types of input models to apply appropriate prediction methods and average the predicted results of individual models to generate final predictions.
4.	Test our ensemble algorithms and conclude that the performance of the balanced random forest which ensembles 375 classification trees is the best and should be used for the CTR prediction.
# Findings After using our self-build ensemble algorithm, we are able to not only increase the prediction accuracy but also balance the performance of our model. We have reached an accuracy of 65.2%, sensitivity of 63.8% and specificity of 65.5% from ensembling 375 classification tree models. Our model can successfully identify 63.8% clicked ads and 65.5% non-clicked ads.
# Conclusion CTR plays a crucial role in measuring the impact of companiesâ€™ advertisements and has become an effective method of precisely targeting customers. Therefore, CTR prediction will be a hot topic in the coming decades. Our project implements CTR prediction by developing and evaluating different models, including Bagging, Random Forest, Gradient Boosting, XGB, Naive Bayes, Classification Tree, KNN and self-build ensemble models. Results show that our self-build ensemble tree based model outperformed all the other models with an accuracy of 65.2% and less than 2% variability among three matrices. 
These results are quite significant considering the computational and time limitations of our research, we have gradually increased the prediction accuracy and dramatically decreased the variability among different performance metrics. Moreover, since we have already built the entire automated modeling system, we can simply sample more training data and include more models in our ensemble algorithm to further boost the performance in the future. Other methods to boost the performance include implementing deep learning algorithms or processing the data on big data platforms like Hadoop and Spark. 
